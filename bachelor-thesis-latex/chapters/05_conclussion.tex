% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Conclusion and Outlook}\label{chapter:conclusion}

In this work a python controller using a 2-layer SNN was implemented based on controller used in \cite{1} to drive a Pioneer robot. It was altered to allow reinforcement learning and control based on additive and multiplicative reward-modulated STDP rules of a snake-like robot without knowledge of external world's model other then the sensory inputs provided by DVS and proximity sensors. 5 proximity sensors were used to provide reward signal, with 2 different calculation methods. The aim was to mimic biologically plausible scenario of autonomous locomotion. Consequently, the training and testing performance of ARM-STDP and MRM-STDP was reviewed. While there was no clear winner in terms of testing performance, since each configuration was able to run for 50000 simulation steps without colliding with a wall, training data shows that MRM-STDP rule is able to learn marginally faster. Area-based reward function performed stabler then center-approximating function. In conclusion, relatively simple model is able to learn autonomous locomotion task correlating its own sensory inputs and not relying on external reward signal. Additive and multiplicative reward-modulation schemes produce equivalent networks. Further improvements of reward function might improve learning significantly - for example, during experiments, a function using 9 proximity sensors was considered to deliver a interpolated wall silhouette, calculate the corresponding areas and use their difference as variable for cubic reward function. This approach was however left behind due to its complicating the snake model. Another interesting topic for further examination is the adaptation of method developed by Nakano et al. in \cite{20}. 